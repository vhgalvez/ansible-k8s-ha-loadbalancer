# üß∞ HAProxy + Keepalived Deployment (Ansible)

---

## üß± Overview: What Are You Building?

You are building a high-availability Kubernetes environment on KVM virtual machines hosted on an HP ProLiant physical server. The setup includes K3s, HAProxy + Keepalived, Traefik as an internal Ingress, distributed storage with Longhorn + NFS, all secured with WireGuard VPN and nftables.

---

## üåê Network Architecture and External Access

```plaintext
[Public Users]
       ‚îÇ
       ‚ñº
+-------------------+
| Cloudflare CDN    | ‚óÑ‚îÄ‚îÄ Proxy + HTTPS + WAF
| (example.com)     |
+-------------------+
       ‚îÇ
       ‚ñº
+----------------------------+
| VPS with Public IP         |
| WireGuard Gateway          |
| Tunnel: 10.17.0.1          |
+----------------------------+
       ‚îÇ
       ‚ñº
+-----------------------------+
| WireGuard Server LAN       |
| NAT + VPN (192.168.0.30)   |
+-----------------------------+
       ‚îÇ
       ‚ñº
Internal Traffic Redirected Based on Type
```

### üéØ Traffic Segmentation in Production

| Traffic Type         | Assigned VIP | Function                                                |
| -------------------- | ------------ | ------------------------------------------------------ |
| Kubernetes API (6443) | 10.17.5.10   | Ensures stability for kubectl, etcd, control-plane     |
| Ingress HTTP/HTTPS    | 10.17.5.30   | Redirects traffic to internal services via Traefik     |

These virtual IPs (VIPs) are managed by HAProxy + Keepalived and automatically switch between nodes.

---

## üß† High-Availability Load Balancers

| Node          | IP           | Function                |
| ------------- | ------------ | ---------------------- |
| k8s-api-lb    | 192.168.0.30 | Main VIP node with br0 bridge |
| loadbalancer1 | 10.17.3.12   | HAProxy backup         |
| loadbalancer2 | 10.17.3.13   | HAProxy backup         |

The three nodes have HAProxy + Keepalived installed. The VIPs 10.17.5.10 (API) and 10.17.5.30 (Ingress) are floating, with only one node maintaining them at a time based on priority.

---

## ‚ò∏Ô∏è Kubernetes Cluster (K3s HA)

| Hostname | IP           | Role                  |
| -------- | ------------ | -------------------- |
| master1  | 10.17.4.21   | etcd + API           |
| master2  | 10.17.4.22   | etcd                 |
| master3  | 10.17.4.23   | etcd                 |
| worker1  | 10.17.4.24   | Application node     |
| worker2  | 10.17.4.25   | Application node     |
| worker3  | 10.17.4.26   | Application node     |

All nodes use Flatcar Container Linux. The K3s cluster operates in etcd HA mode. It is installed with `--tls-san 10.17.5.10` to allow `kubectl` access via the VIP.

---

## üö™ Ingress Controller (Traefik)

| Type    | Deployment                       |
| ------- | -------------------------------- |
| Traefik | Internal deployment in Kubernetes |

Access is through the VIP `10.17.5.30` managed by HAProxy. Traefik communicates with pods via ClusterIP and is not directly exposed.

---

## üíæ Persistent Storage

| Node     | IP           | Role            |
| -------- | ------------ | -------------- |
| storage1 | 10.17.4.27   | NFS + Longhorn |

**Longhorn (RWO):**

- Microservices
- Prometheus
- Grafana
- ELK

**NFS (RWX):**

- PostgreSQL ‚Üí `/srv/nfs/postgresql`
- Shared data ‚Üí `/srv/nfs/shared`

---

## üîê Security

| Component    | Details                                |
| ------------- | --------------------------------------- |
| WireGuard     | Secure remote access from the VPS       |
| nftables      | Strict firewall on the physical server |
| Cloudflare    | HTTPS, WAF, DDoS protection            |
| Authentication | basicAuth for internal dashboards     |
| DNS/NTP       | intra-cluster (`10.17.3.11`)           |

---

## üß† Automation and CI/CD

| Tool           | Function                                |
| -------------- | -------------------------------------- |
| Terraform      | VM and network provisioning            |
| Ansible        | Installation and configuration (100% IaC) |
| Jenkins + ArgoCD | Internal CI/CD                        |

---

## üñ• Machine Table

| Hostname      | IP           | Function                     |
| ------------- | ------------ | --------------------------- |
| master1       | 10.17.4.21   | K3s Master + etcd           |
| master2       | 10.17.4.22   | K3s Master + etcd           |
| master3       | 10.17.4.23   | K3s Master + etcd           |
| worker1       | 10.17.4.24   | Application node            |
| worker2       | 10.17.4.25   | Application node            |
| worker3       | 10.17.4.26   | Application node            |
| storage1      | 10.17.4.27   | Longhorn + NFS              |
| k8s-api-lb    | 192.168.0.30 | HAProxy + Keepalived (VIPs) |
| loadbalancer1 | 10.17.3.12   | HAProxy (backup)            |
| loadbalancer2 | 10.17.3.13   | HAProxy (backup)            |
| postgresql1   | 10.17.3.14   | Centralized PostgreSQL      |
| infra-cluster | 10.17.3.11   | CoreDNS + Chrony            |

---

## ‚úÖ Advantages of This Architecture

- True high availability with multiple separated VIPs.
- Internally controlled ingress with Traefik.
- Robust security via VPN, nftables, and HTTPS.
- Full automation (Terraform + Ansible).
- Distributed and fault-tolerant storage.
- Modular design for growth without redesign.

---

# üß∞ Documentation: Bootstrap of K3s Cluster with HAProxy + Keepalived + VIPs

## üìÑ Objective

Allow the `master1` node to bootstrap the K3s cluster without requiring HAProxy or Keepalived to be active and functional beforehand. This resolves the classic cyclic dependency problem ("the egg or the chicken") when using a VIP (virtual IP) as the entry point to the cluster.

---

## üèõÔ∏è Architecture

- **API Server VIP**: `10.17.5.10`
- **Ingress VIP**: `10.17.5.30`
- **Masters**:
  - `10.17.4.21` (bootstrap)
  - `10.17.4.22`
  - `10.17.4.23`
- **Workers**:
  - `10.17.4.24`, `10.17.4.25`, `10.17.4.26`, `10.17.4.27`
- **Load Balancers**:
  - `10.17.3.12`, `10.17.3.13`, `192.168.0.30`

---


k8s-api-lb 192.168.0.30 se crea un adaptador de puente `br0` para que los nodos puedan comunicarse entre s√≠ y con el mundo exterior.

## üîÑ Orden de inicio esperado

1. El nodo `master1` se inicializa con su IP real (`10.17.4.21`).
2. Se levanta el `k3s-server` y el `etcd` en `master1`.
3. Los otros masters se unen usando `https://10.17.4.21:6443` (no la VIP).
4. Una vez el cl√∫ster est√° operativo:

   * Se configura la VIP (`10.17.5.10`) con Keepalived.
   * Se habilita HAProxy en los nodos `haproxy_keepalived`.
5. HAProxy redirige el tr√°fico de `10.17.5.10:6443` hacia los masters disponibles.
6. El `kubeconfig` puede comenzar a usar la VIP como endpoint oficial.

---

## ‚úÖ Configuraci√≥n correcta para romper el ciclo

### 1. **`master1` usa su IP real para bootstrap**

* El script de Ansible no apunta a la VIP (`10.17.5.10`) para levantar el nodo inicial.
* Esto permite iniciar el API Server antes que HAProxy.

### 2. **HAProxy permite `bind` en IPs no locales**

```yaml
- name: Habilitar net.ipv4.ip_nonlocal_bind
  ansible.posix.sysctl:
    name: net.ipv4.ip_nonlocal_bind
    value: "1"
    sysctl_file: /etc/sysctl.d/99-haproxy-nonlocal-bind.conf
    reload: yes
    state: present
```

Esto evita errores de HAProxy como `Cannot bind to VIP`, ya que permite iniciar el proceso sin que la IP est√© asignada localmente a la interfaz.

### 3. **Keepalived no requiere HAProxy para iniciar**

```ini
# override.conf
[Unit]
After=haproxy.service
# NO incluye Requires=haproxy.service
```

Esto asegura que Keepalived pueda arrancar independientemente de HAProxy. La relaci√≥n es suave, no bloqueante.

### 4. **VIP solo se usa despu√©s de la estabilizaci√≥n**

* El uso de la VIP para el `kubeconfig` solo se hace despu√©s de validar que el balanceador HAProxy est√© activo.

### 5. **Configuraci√≥n de HAProxy**

El `haproxy.cfg` est√° correctamente estructurado para enrutar tr√°fico TCP en el puerto 6443 hacia los masters:

```haproxy
frontend kubernetes_api
    bind 10.17.5.10:6443
    mode tcp
    option tcplog
    default_backend kubernetes_masters

backend kubernetes_masters
    mode tcp
    balance roundrobin
    option tcp-check
    tcp-check connect port 6443
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server master-1 10.17.4.21:6443 check
    server master-2 10.17.4.22:6443 check
    server master-3 10.17.4.23:6443 check
```

---

## üîß Validaciones adicionales

* Comando para verificar si HAProxy permite bind:

```bash
sysctl net.ipv4.ip_nonlocal_bind
```

* Verificar configuraci√≥n de HAProxy:

```bash
haproxy -c -f /etc/haproxy/haproxy.cfg
```

* Verificar si est√° corriendo:

```bash
sudo systemctl status haproxy
```

---

## üß™ Estado de los Balanceadores tras el Playbook `install_haproxy_keepalived.yml`

Este es el estado esperado de los nodos balanceadores una vez finaliza la instalaci√≥n autom√°tica con Ansible. Todos los nodos tienen HAProxy + Keepalived configurados, y las VIPs se asignan autom√°ticamente por prioridad.

| Hostname        | IP           | Rol                                 | Keepalived         | HAProxy             | VIPs Activas                      |
|-----------------|--------------|--------------------------------------|--------------------|---------------------|------------------------------------|
| `k8s-api-lb`    | `192.168.0.30` | Nodo principal de VIPs (`priority=100`) | ‚úÖ Activo (MASTER)  | ‚úÖ Activo y corriendo | ‚úÖ `10.17.5.10` y `10.17.5.30`      |
| `loadbalancer1` | `10.17.3.12` | Respaldo 1 (`priority=120`)         | ‚úÖ Activo (BACKUP)  | ‚úÖ Activo (en espera) | ‚ùå (asumir√° VIPs si el principal cae) |
| `loadbalancer2` | `10.17.3.13` | Respaldo 2 (`priority=110`)         | ‚úÖ Activo (BACKUP)  | ‚úÖ Activo (en espera) | ‚ùå (asumir√° VIPs si los anteriores caen) |

---

### ‚öôÔ∏è Detalles t√©cnicos

- Todos los nodos tienen `HAProxy` habilitado y en ejecuci√≥n (`enabled + running`).
- Todos usan `ip_nonlocal_bind=1` para permitir el arranque sin poseer la VIP localmente.
- Keepalived gestiona la flotaci√≥n de las siguientes IPs virtuales:
  - `10.17.5.10` ‚Üí Kubernetes API (`6443`)
  - `10.17.5.30` ‚Üí Ingress HTTP/HTTPS (`80` y `443`)
- El nodo que obtiene las VIPs es determinado por el archivo `host_vars/<ip>.yml` con sus prioridades:
  - `keepalived_priority_api`
  - `keepalived_priority_ingress`
- Si el nodo principal falla, el siguiente en prioridad **asume autom√°ticamente las VIPs** y el tr√°fico contin√∫a sin interrupciones.

---


## üéØ Conclusi√≥n

Con esta configuraci√≥n:

* El nodo `master1` puede iniciar el cl√∫ster sin la VIP.
* Los nodos balanceadores y Keepalived pueden arrancar sin romper dependencias.
* HAProxy puede arrancar incluso si no posee la VIP.

üëç Est√°s aplicando correctamente un patr√≥n de alta disponibilidad tolerante a fallos y circularidades de dependencia.



ansible-galaxy collection install community.general

üß∞ Resumen del Proyecto: HAProxy + Keepalived para K3s HA
Este proyecto implementa una soluci√≥n de balanceo de carga altamente disponible para el acceso al cl√∫ster Kubernetes mediante HAProxy y Keepalived, gestionando m√∫ltiples VIPs (IP Virtuales) para separar tr√°fico cr√≠tico del API y del Ingress HTTP/HTTPS.

üéØ Objetivo
Garantizar:

Acceso ininterrumpido al API de Kubernetes (puerto 6443).

Disponibilidad continua para tr√°fico HTTP/HTTPS hacia los servicios internos (Ingress).

Failover autom√°tico de las IPs virtuales entre m√∫ltiples nodos balanceadores.

üîß Componentes Clave
Componente	Descripci√≥n
HAProxy	Balanceador de carga TCP/HTTP para API y tr√°fico web
Keepalived	Gestor de alta disponibilidad mediante VRRP para mover VIPs entre nodos
VIPs	IPs flotantes que garantizan un √∫nico punto de entrada para el tr√°fico
Ansible	Automatizaci√≥n completa del despliegue y configuraci√≥n
K3s	Cl√∫ster Kubernetes ligero y altamente disponible

üåê Arquitectura General de Red
bash
Copiar
Editar
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ        Clientes externos    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ Cloudflare‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                       VPN / NAT / WireGuard
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Load Balancer Principal‚îÇ  <- VIPs: 10.17.5.10 / 10.17.5.30
                    ‚îÇ   (HAProxy + Keepalived)‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                                   ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ  K3s Master #1  ‚îÇ               ‚îÇ  K3s Master #2-3    ‚îÇ
     ‚îÇ API + etcd      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ API + etcd           ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

              ‚îÇ Ingress tr√°fico HTTP/HTTPS via Traefik
              ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Workers   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
üì¶ VIPs y Tr√°fico
Tipo de Tr√°fico	Puerto(s)	VIP	Destino final
API de Kubernetes	6443	10.17.5.10	Nodos master de K3s (HA)
Servicios Ingress	80 / 443	10.17.5.30	Nodos worker v√≠a Traefik (interno)

Las VIPs son asignadas din√°micamente al nodo con mayor prioridad activa.

Si un nodo falla, Keepalived transfiere la VIP al siguiente disponible.

üõ†Ô∏è Mecanismo de Alta Disponibilidad
HAProxy:

Act√∫a como proxy TCP para 6443 y como proxy HTTP para 80/443.

Verifica salud de los nodos K3s y trabajadores.

Permite configuraci√≥n nonlocal_bind para bindear IPs no locales.

Keepalived:

Ejecuta VRRP y scripts de tracking (estado de HAProxy).

Usa prioridad para determinar nodo MASTER de las VIPs.

Ejecuta vip_master.sh, vip_backup.sh, vip_fault.sh seg√∫n evento.

Ansible:

Automatiza instalaci√≥n en nodos HA.

Configura todos los archivos .cfg, .service, .conf necesarios.

Detecta Flatcar y aplica configuraciones especiales si es necesario.

üìë Flujo de Implementaci√≥n con Ansible
Detecta distribuci√≥n (Flatcar o no).

Instala HAProxy, Keepalived y dependencias.

Configura sysctl para permitir nonlocal_bind.

Aplica configuraciones plantilladas (haproxy.cfg.j2, keepalived.conf.j2).

Configura override de systemd para evitar dependencias circulares.

Reinicia servicios y valida salud.

‚úÖ Ventajas Clave
Alta disponibilidad real (failover autom√°tico).

Separaci√≥n de tr√°fico cr√≠tico.

Escalabilidad horizontal simple.

Configuraci√≥n 100% automatizada y auditable (IaC).

Seguridad de acceso por VPN y Cloudflare (si aplica).
---
ansible-k8s-ha-loadbalancer/
‚îú‚îÄ‚îÄ ansible.cfg
‚îú‚îÄ‚îÄ inventory/
‚îÇ   ‚îî‚îÄ‚îÄ hosts.ini
‚îú‚îÄ‚îÄ host_vars/
‚îÇ   ‚îú‚îÄ‚îÄ 10.17.3.12.yml        # loadbalancer1
‚îÇ   ‚îú‚îÄ‚îÄ 10.17.3.13.yml        # loadbalancer2
‚îÇ   ‚îî‚îÄ‚îÄ 192.168.0.30.yml        # k8s-api-lb (nodo principal)
‚îú‚îÄ‚îÄ playbooks/
‚îÇ   ‚îî‚îÄ‚îÄ install_haproxy_keepalived.yml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ haproxy/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ haproxy.cfg.j2
‚îÇ   ‚îî‚îÄ‚îÄ keepalived/
‚îÇ       ‚îî‚îÄ‚îÄ keepalived.conf.j2
‚îú‚îÄ‚îÄ Makefile
‚îî‚îÄ‚îÄ README.md
---




üñ•Ô∏è Tabla de M√°quinas y Servicios
Hostname	IP	Rol	Servicio	Estado Esperado	Comentario
k8s-api-lb	192.168.0.30	Nodo principal de balanceo	haproxy	üü¢ Activo	Nodo que deber√≠a mantener las VIPs activas (por prioridad m√°s baja)
keepalived	üü¢ Activo (MASTER)	Controla VIPs 10.17.5.10 (API) y 10.17.5.30 (Ingress)
loadbalancer1	10.17.3.12	Nodo de respaldo 1 de balanceo	haproxy	üü¢ Activo	Nodo backup, asume VIPs si k8s-api-lb cae
keepalived	üü¢ Activo (BACKUP)	Se convierte en MASTER si el nodo principal falla
loadbalancer2	10.17.3.13	Nodo de respaldo 2 de balanceo	haproxy	üü¢ Activo	Segundo backup, entra si ambos anteriores fallan
keepalived	üü¢ Activo (BACKUP)	Estado pasivo, listo para asumir en caso de emergencia
master1	10.17.4.21	Kubernetes API + etcd + bootstrap	k3s server	üü¢ Activo	Primer nodo que inicia el cluster sin necesidad de VIP
master2	10.17.4.22	Kubernetes API + etcd	k3s server	üü¢ Activo	Se une al cl√∫ster v√≠a IP real o API VIP
master3	10.17.4.23	Kubernetes API + etcd	k3s server	üü¢ Activo	Parte del quorum de etcd
worker1	10.17.4.24	Nodo de aplicaci√≥n	k3s agent	üü¢ Activo	Recibe tr√°fico HTTP/HTTPS v√≠a Traefik + VIP Ingress
worker2	10.17.4.25	Nodo de aplicaci√≥n	k3s agent	üü¢ Activo	Balanceado por Traefik
worker3	10.17.4.26	Nodo de aplicaci√≥n	k3s agent	üü¢ Activo	Balanceado por Traefik
storage1	10.17.4.27	NFS + Longhorn	nfs-server	üü¢ Activo	Montado como RWX (PostgreSQL, compartido) y RWO (Longhorn)
infra-cluster	10.17.3.11	DNS (CoreDNS), NTP (Chrony)	dns, ntp	üü¢ Activo	Servidor de infraestructura para sincron√≠a y resoluci√≥n interna
postgresql1	10.17.3.14	Base de datos centralizada	postgresql	üü¢ Activo	Puede estar montado en NFS compartido

üéØ Comportamiento de Failover de Keepalived
üß† VIP 10.17.5.10 (API Server)
Asignada por defecto al nodo k8s-api-lb

Si este cae:

loadbalancer1 detecta la ca√≠da y asume la IP VIP

Si tambi√©n cae, loadbalancer2 asume la IP

‚û°Ô∏è El acceso al API (puerto 6443) sigue funcionando sin interrupciones para kubectl, etcd, y kubelet.

üåê VIP 10.17.5.30 (Ingress HTTP/HTTPS)
Tambi√©n manejada por k8s-api-lb

Redirige tr√°fico HTTP/HTTPS (puertos 80/443) hacia los pods (Traefik interno)

Failover autom√°tico entre los tres balanceadores seg√∫n prioridad

‚û°Ô∏è El tr√°fico web externo es reenviado correctamente a trav√©s del Ingress aunque un nodo de balanceo falle.

üìä Resumen de Estados Esperados

| Nodo            | Servicio    | Estado         | Observaciones                              |
|-----------------|-------------|----------------|-------------------------------------------|
| k8s-api-lb      | haproxy     | ‚úÖ corriendo   | Posee ambas VIPs (por prioridad)          |
|                 | keepalived  | ‚úÖ corriendo   | Estado MASTER                             |
| loadbalancer1   | haproxy     | ‚úÖ corriendo   | Espera en BACKUP                          |
|                 | keepalived  | ‚úÖ corriendo   | BACKUP con menor prioridad                |
| loadbalancer2   | haproxy     | ‚úÖ corriendo   | Espera en BACKUP                          |
|                 | keepalived  | ‚úÖ corriendo   | BACKUP                                    |

## üì¶ Importante sobre HAProxy

- Requiere `net.ipv4.ip_nonlocal_bind = 1` para aceptar conexiones en IPs VIP que no est√©n asignadas localmente.
- Se arranca incluso si la VIP no est√° disponible a√∫n (por dise√±o de HA).
- Las configuraciones est√°n correctamente desacopladas gracias al override systemd y `After=haproxy.service`.

## ‚úÖ Conclusiones

- Tu dise√±o es resiliente, modular y de alta disponibilidad real.
- El cl√∫ster no depende de las VIPs para arrancar, lo cual rompe el ciclo ‚Äúhuevo-gallina‚Äù.
- En caso de falla de cualquier balanceador, los otros asumen sin intervenci√≥n humana.
- La infraestructura est√° lista para producci√≥n y escalamiento.

# üì¶ Instalaci√≥n de HAProxy y Keepalived

```bash
sudo ansible-playbook ansible/playbooks/setup_haproxy_keepalived_full.yml -i inventory/hosts.ini
```
