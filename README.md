# ğŸ§° HAProxy + Keepalived Deployment (Ansible)

Este repositorio despliega un balanceador de carga con alta disponibilidad usando **HAProxy** y **Keepalived** para clÃºsteres Kubernetes (K3s o Kubernetes tradicionales). Proporciona balanceo de trÃ¡fico en las capas TCP/HTTP y maneja mÃºltiples VIPs para separar trÃ¡fico del API y del Ingress.

---

## ğŸ§± VisiÃ³n General: Â¿QuÃ© estÃ¡s construyendo?

EstÃ¡s construyendo un entorno Kubernetes de alta disponibilidad sobre mÃ¡quinas virtuales KVM en un servidor fÃ­sico HP ProLiant, utilizando K3s, HAProxy + Keepalived, Traefik como Ingress interno, y almacenamiento distribuido con Longhorn + NFS, todo asegurado mediante VPN WireGuard y nftables.

---

## ğŸŒ Arquitectura de Red y Accesos Externos

```
[Usuarios PÃºblicos]
       â”‚
       â–¼
+-------------------+
| Cloudflare CDN    | â—„â”€â”€ Proxy + HTTPS + WAF
| (example.com)     |
+-------------------+
       â”‚
       â–¼
+----------------------------+
| VPS con IP pÃºblica         |
| WireGuard Gateway          |
| TÃºnel: 10.17.0.1           |
+----------------------------+
       â”‚
       â–¼
+-----------------------------+
| WireGuard Server LAN       |
| NAT + VPN (192.168.0.19)   |
+-----------------------------+
       â”‚
       â–¼
TrÃ¡fico Interno Redirigido SegÃºn Tipo
```

### ğŸ¯ SeparaciÃ³n de TrÃ¡fico en ProducciÃ³n

| Tipo de TrÃ¡fico       | VIP Asignada | FunciÃ³n                                                |
| --------------------- | ------------ | ------------------------------------------------------ |
| Kubernetes API (6443) | 10.17.5.10   | Requiere estabilidad para kubectl, etcd, control-plane |
| Ingress HTTP/HTTPS    | 10.17.5.30   | Redirige trÃ¡fico a servicios internos vÃ­a Traefik      |

Estas IPs virtuales (VIP) son gestionadas por HAProxy + Keepalived y conmutan entre nodos automÃ¡ticamente.

### ğŸ§  Balanceadores HA

| Nodo          | IP         | FunciÃ³n                |
| ------------- | ---------- | ---------------------- |
| k8s-api-lb    | 10.17.5.20 | Nodo principal de VIPs |
| loadbalancer1 | 10.17.3.12 | Respaldo HAProxy       |
| loadbalancer2 | 10.17.3.13 | Respaldo HAProxy       |

Los tres tienen HAProxy + Keepalived instalados.

Las VIPs 10.17.5.10 (API) y 10.17.5.30 (Ingress) son flotantes. Solo un nodo las mantiene activas al mismo tiempo (por prioridad).

---

## â˜¸ï¸ ClÃºster Kubernetes (K3s HA)

| Hostname | IP         | Rol                  |
| -------- | ---------- | -------------------- |
| master1  | 10.17.4.21 | etcd + API           |
| master2  | 10.17.4.22 | etcd                 |
| master3  | 10.17.4.23 | etcd                 |
| worker1  | 10.17.4.24 | Nodo de aplicaciones |
| worker2  | 10.17.4.25 | Nodo de aplicaciones |
| worker3  | 10.17.4.26 | Nodo de aplicaciones |

Todos los nodos usan Flatcar Container Linux. ClÃºster K3s en modo etcd HA.

Se instala con `--tls-san 10.17.5.10` para que `kubectl` acceda vÃ­a la VIP.

---

## ğŸšª Ingress Controller (Traefik)

| Tipo    | Despliegue                       |
| ------- | -------------------------------- |
| Traefik | Deployment interno en Kubernetes |

El acceso es a travÃ©s de la VIP `10.17.5.30` gestionada por HAProxy. Traefik se comunica con los pods vÃ­a ClusterIP. No se expone directamente.

---

## ğŸ’¾ Almacenamiento Persistente

| Nodo     | IP         | Rol            |
| -------- | ---------- | -------------- |
| storage1 | 10.17.4.27 | NFS + Longhorn |

**Longhorn (RWO):**

* Microservicios
* Prometheus
* Grafana
* ELK

**NFS (RWX):**

* PostgreSQL â†’ `/srv/nfs/postgresql`
* Datos compartidos â†’ `/srv/nfs/shared`

---

## ğŸ” Seguridad

| Componente    | Detalles                                |
| ------------- | --------------------------------------- |
| WireGuard     | Acceso remoto seguro desde el VPS       |
| nftables      | Firewall estricto en el servidor fÃ­sico |
| Cloudflare    | HTTPS, WAF, ProtecciÃ³n contra DDoS      |
| AutenticaciÃ³n | basicAuth en dashboards internos        |
| DNS/NTP       | infra-cluster (`10.17.3.11`)            |

---

## ğŸ§  AutomatizaciÃ³n y CI/CD

| Herramienta      | FunciÃ³n                                |
| ---------------- | -------------------------------------- |
| Terraform        | ProvisiÃ³n de VMs y redes               |
| Ansible          | InstalaciÃ³n y configuraciÃ³n (100% IaC) |
| Jenkins + ArgoCD | CI/CD interno                          |

---

## ğŸ–¥ Tabla de MÃ¡quinas

| Hostname      | IP         | FunciÃ³n                     |
| ------------- | ---------- | --------------------------- |
| master1       | 10.17.4.21 | K3s Master + etcd           |
| master2       | 10.17.4.22 | K3s Master + etcd           |
| master3       | 10.17.4.23 | K3s Master + etcd           |
| worker1       | 10.17.4.24 | Nodo de aplicaciones        |
| worker2       | 10.17.4.25 | Nodo de aplicaciones        |
| worker3       | 10.17.4.26 | Nodo de aplicaciones        |
| storage1      | 10.17.4.27 | Longhorn + NFS              |
| k8s-api-lb    | 10.17.5.20 | HAProxy + Keepalived (VIPs) |
| loadbalancer1 | 10.17.3.12 | HAProxy (respaldo)          |
| loadbalancer2 | 10.17.3.13 | HAProxy (respaldo)          |
| postgresql1   | 10.17.3.14 | PostgreSQL centralizado     |
| infra-cluster | 10.17.3.11 | CoreDNS + Chrony            |

---

## âœ… Ventajas de esta Arquitectura

* ğŸ” Alta disponibilidad real con mÃºltiples VIPs separadas.
* ğŸšª Ingress controlado internamente con Traefik.
* ğŸ›¡ï¸ Seguridad robusta por VPN, nftables y HTTPS.
* ğŸ§° AutomatizaciÃ³n total (Terraform + Ansible).
* ğŸ“¦ Almacenamiento distribuido y tolerante a fallos.
* ğŸ§± Modularidad para crecer sin rediseÃ±ar.

sudo ansible-playbook -i inventory/hosts.ini ansible/playbooks/install_haproxy_keepalived.yml


sudo ansible-playbook -i inventory/hosts.ini ansible/playbooks/uninstall_haproxy_keepalived.yml


# ğŸ§° DocumentaciÃ³n: Bootstrap de ClÃºster K3s con HAProxy + Keepalived + VIPs

## ğŸ“„ Objetivo

Permitir que el nodo `master1` pueda iniciar el clÃºster K3s sin depender previamente de que HAProxy o Keepalived estÃ©n activos y funcionales. Esto resuelve el clÃ¡sico problema de dependencia cÃ­clica ("el huevo o la gallina") al usar una VIP (IP virtual) como punto de entrada al clÃºster.

---

## ğŸ›ï¸ Arquitectura

* **VIP del API Server**: `10.17.5.10`
* **VIP del Ingress**: `10.17.5.30`
* **Masters**:

  * `10.17.4.21` (bootstrap)
  * `10.17.4.22`
  * `10.17.4.23`
* **Workers**:

  * `10.17.4.24`, `10.17.4.25`, `10.17.4.26`, `10.17.4.27`
* **Load Balancers**:

  * `10.17.3.12`, `10.17.3.13`, `10.17.5.20`

---

## ğŸ”„ Orden de inicio esperado

1. El nodo `master1` se inicializa con su IP real (`10.17.4.21`).
2. Se levanta el `k3s-server` y el `etcd` en `master1`.
3. Los otros masters se unen usando `https://10.17.4.21:6443` (no la VIP).
4. Una vez el clÃºster estÃ¡ operativo:

   * Se configura la VIP (`10.17.5.10`) con Keepalived.
   * Se habilita HAProxy en los nodos `haproxy_keepalived`.
5. HAProxy redirige el trÃ¡fico de `10.17.5.10:6443` hacia los masters disponibles.
6. El `kubeconfig` puede comenzar a usar la VIP como endpoint oficial.

---

## âœ… ConfiguraciÃ³n correcta para romper el ciclo

### 1. **`master1` usa su IP real para bootstrap**

* El script de Ansible no apunta a la VIP (`10.17.5.10`) para levantar el nodo inicial.
* Esto permite iniciar el API Server antes que HAProxy.

### 2. **HAProxy permite `bind` en IPs no locales**

```yaml
- name: Habilitar net.ipv4.ip_nonlocal_bind
  ansible.posix.sysctl:
    name: net.ipv4.ip_nonlocal_bind
    value: "1"
    sysctl_file: /etc/sysctl.d/99-haproxy-nonlocal-bind.conf
    reload: yes
    state: present
```

Esto evita errores de HAProxy como `Cannot bind to VIP`, ya que permite iniciar el proceso sin que la IP estÃ© asignada localmente a la interfaz.

### 3. **Keepalived no requiere HAProxy para iniciar**

```ini
# override.conf
[Unit]
After=haproxy.service
# NO incluye Requires=haproxy.service
```

Esto asegura que Keepalived pueda arrancar independientemente de HAProxy. La relaciÃ³n es suave, no bloqueante.

### 4. **VIP solo se usa despuÃ©s de la estabilizaciÃ³n**

* El uso de la VIP para el `kubeconfig` solo se hace despuÃ©s de validar que el balanceador HAProxy estÃ© activo.

### 5. **ConfiguraciÃ³n de HAProxy**

El `haproxy.cfg` estÃ¡ correctamente estructurado para enrutar trÃ¡fico TCP en el puerto 6443 hacia los masters:

```haproxy
frontend kubernetes_api
    bind 10.17.5.10:6443
    mode tcp
    option tcplog
    default_backend kubernetes_masters

backend kubernetes_masters
    mode tcp
    balance roundrobin
    option tcp-check
    tcp-check connect port 6443
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server master-1 10.17.4.21:6443 check
    server master-2 10.17.4.22:6443 check
    server master-3 10.17.4.23:6443 check
```

---

## ğŸ”§ Validaciones adicionales

* Comando para verificar si HAProxy permite bind:

```bash
sysctl net.ipv4.ip_nonlocal_bind
```

* Verificar configuraciÃ³n de HAProxy:

```bash
haproxy -c -f /etc/haproxy/haproxy.cfg
```

* Verificar si estÃ¡ corriendo:

```bash
sudo systemctl status haproxy
```

---

## ğŸ¯ ConclusiÃ³n

Con esta configuraciÃ³n:

* El nodo `master1` puede iniciar el clÃºster sin la VIP.
* Los nodos balanceadores y Keepalived pueden arrancar sin romper dependencias.
* HAProxy puede arrancar incluso si no posee la VIP.

ğŸ‘ EstÃ¡s aplicando correctamente un patrÃ³n de alta disponibilidad tolerante a fallos y circularidades de dependencia.



ansible-galaxy collection install community.general


---
ansible-k8s-ha-loadbalancer/
â”œâ”€â”€ ansible.cfg
â”œâ”€â”€ inventory/
â”‚   â””â”€â”€ hosts.ini
â”œâ”€â”€ host_vars/
â”‚   â”œâ”€â”€ 10.17.3.12.yml        # loadbalancer1
â”‚   â”œâ”€â”€ 10.17.3.13.yml        # loadbalancer2
â”‚   â””â”€â”€ 10.17.5.20.yml        # k8s-api-lb (nodo principal)
â”œâ”€â”€ playbooks/
â”‚   â””â”€â”€ install_haproxy_keepalived.yml
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ haproxy/
â”‚   â”‚   â””â”€â”€ haproxy.cfg.j2
â”‚   â””â”€â”€ keepalived/
â”‚       â””â”€â”€ keepalived.conf.j2
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
---