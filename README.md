# ğŸ§° HAProxy + Keepalived Deployment (Ansible)

Este repositorio despliega un balanceador de carga con alta disponibilidad usando **HAProxy** y **Keepalived** para clÃºsteres Kubernetes (K3s o Kubernetes tradicionales). Proporciona balanceo de trÃ¡fico en las capas TCP/HTTP y maneja mÃºltiples VIPs para separar trÃ¡fico del API y del Ingress.

---

## ğŸ§± VisiÃ³n General: Â¿QuÃ© estÃ¡s construyendo?

EstÃ¡s construyendo un entorno Kubernetes de alta disponibilidad sobre mÃ¡quinas virtuales KVM en un servidor fÃ­sico HP ProLiant, utilizando K3s, HAProxy + Keepalived, Traefik como Ingress interno, y almacenamiento distribuido con Longhorn + NFS, todo asegurado mediante VPN WireGuard y nftables.

---

## ğŸŒ Arquitectura de Red y Accesos Externos

```
[Usuarios PÃºblicos]
       â”‚
       â–¼
+-------------------+
| Cloudflare CDN    | â—„â”€â”€ Proxy + HTTPS + WAF
| (example.com)     |
+-------------------+
       â”‚
       â–¼
+----------------------------+
| VPS con IP pÃºblica         |
| WireGuard Gateway          |
| TÃºnel: 10.17.0.1           |
+----------------------------+
       â”‚
       â–¼
+-----------------------------+
| WireGuard Server LAN       |
| NAT + VPN (192.168.0.19)   |
+-----------------------------+
       â”‚
       â–¼
TrÃ¡fico Interno Redirigido SegÃºn Tipo
```

### ğŸ¯ SeparaciÃ³n de TrÃ¡fico en ProducciÃ³n

| Tipo de TrÃ¡fico       | VIP Asignada | FunciÃ³n                                                |
| --------------------- | ------------ | ------------------------------------------------------ |
| Kubernetes API (6443) | 10.17.5.10   | Requiere estabilidad para kubectl, etcd, control-plane |
| Ingress HTTP/HTTPS    | 10.17.5.30   | Redirige trÃ¡fico a servicios internos vÃ­a Traefik      |

Estas IPs virtuales (VIP) son gestionadas por HAProxy + Keepalived y conmutan entre nodos automÃ¡ticamente.

### ğŸ§  Balanceadores HA

| Nodo          | IP         | FunciÃ³n                |
| ------------- | ---------- | ---------------------- |
| k8s-api-lb    | 10.17.5.20 | Nodo principal de VIPs |
| loadbalancer1 | 10.17.3.12 | Respaldo HAProxy       |
| loadbalancer2 | 10.17.3.13 | Respaldo HAProxy       |

Los tres tienen HAProxy + Keepalived instalados.

Las VIPs 10.17.5.10 (API) y 10.17.5.30 (Ingress) son flotantes. Solo un nodo las mantiene activas al mismo tiempo (por prioridad).

---

## â˜¸ï¸ ClÃºster Kubernetes (K3s HA)

| Hostname | IP         | Rol                  |
| -------- | ---------- | -------------------- |
| master1  | 10.17.4.21 | etcd + API           |
| master2  | 10.17.4.22 | etcd                 |
| master3  | 10.17.4.23 | etcd                 |
| worker1  | 10.17.4.24 | Nodo de aplicaciones |
| worker2  | 10.17.4.25 | Nodo de aplicaciones |
| worker3  | 10.17.4.26 | Nodo de aplicaciones |

Todos los nodos usan Flatcar Container Linux. ClÃºster K3s en modo etcd HA.

Se instala con `--tls-san 10.17.5.10` para que `kubectl` acceda vÃ­a la VIP.

---

## ğŸšª Ingress Controller (Traefik)

| Tipo    | Despliegue                       |
| ------- | -------------------------------- |
| Traefik | Deployment interno en Kubernetes |

El acceso es a travÃ©s de la VIP `10.17.5.30` gestionada por HAProxy. Traefik se comunica con los pods vÃ­a ClusterIP. No se expone directamente.

---

## ğŸ’¾ Almacenamiento Persistente

| Nodo     | IP         | Rol            |
| -------- | ---------- | -------------- |
| storage1 | 10.17.4.27 | NFS + Longhorn |

**Longhorn (RWO):**

* Microservicios
* Prometheus
* Grafana
* ELK

**NFS (RWX):**

* PostgreSQL â†’ `/srv/nfs/postgresql`
* Datos compartidos â†’ `/srv/nfs/shared`

---

## ğŸ” Seguridad

| Componente    | Detalles                                |
| ------------- | --------------------------------------- |
| WireGuard     | Acceso remoto seguro desde el VPS       |
| nftables      | Firewall estricto en el servidor fÃ­sico |
| Cloudflare    | HTTPS, WAF, ProtecciÃ³n contra DDoS      |
| AutenticaciÃ³n | basicAuth en dashboards internos        |
| DNS/NTP       | infra-cluster (`10.17.3.11`)            |

---

## ğŸ§  AutomatizaciÃ³n y CI/CD

| Herramienta      | FunciÃ³n                                |
| ---------------- | -------------------------------------- |
| Terraform        | ProvisiÃ³n de VMs y redes               |
| Ansible          | InstalaciÃ³n y configuraciÃ³n (100% IaC) |
| Jenkins + ArgoCD | CI/CD interno                          |

---

## ğŸ–¥ Tabla de MÃ¡quinas

| Hostname      | IP         | FunciÃ³n                     |
| ------------- | ---------- | --------------------------- |
| master1       | 10.17.4.21 | K3s Master + etcd           |
| master2       | 10.17.4.22 | K3s Master + etcd           |
| master3       | 10.17.4.23 | K3s Master + etcd           |
| worker1       | 10.17.4.24 | Nodo de aplicaciones        |
| worker2       | 10.17.4.25 | Nodo de aplicaciones        |
| worker3       | 10.17.4.26 | Nodo de aplicaciones        |
| storage1      | 10.17.4.27 | Longhorn + NFS              |
| k8s-api-lb    | 10.17.5.20 | HAProxy + Keepalived (VIPs) |
| loadbalancer1 | 10.17.3.12 | HAProxy (respaldo)          |
| loadbalancer2 | 10.17.3.13 | HAProxy (respaldo)          |
| postgresql1   | 10.17.3.14 | PostgreSQL centralizado     |
| infra-cluster | 10.17.3.11 | CoreDNS + Chrony            |

---

## âœ… Ventajas de esta Arquitectura

* ğŸ” Alta disponibilidad real con mÃºltiples VIPs separadas.
* ğŸšª Ingress controlado internamente con Traefik.
* ğŸ›¡ï¸ Seguridad robusta por VPN, nftables y HTTPS.
* ğŸ§° AutomatizaciÃ³n total (Terraform + Ansible).
* ğŸ“¦ Almacenamiento distribuido y tolerante a fallos.
* ğŸ§± Modularidad para crecer sin rediseÃ±ar.

sudo ansible-playbook -i inventory/hosts.ini ansible/playbooks/install_haproxy_keepalived.yml


sudo ansible-playbook -i inventory/hosts.ini ansible/playbooks/uninstall_haproxy_keepalived.yml


# ğŸ§° DocumentaciÃ³n: Bootstrap de ClÃºster K3s con HAProxy + Keepalived + VIPs

## ğŸ“„ Objetivo

Permitir que el nodo `master1` pueda iniciar el clÃºster K3s sin depender previamente de que HAProxy o Keepalived estÃ©n activos y funcionales. Esto resuelve el clÃ¡sico problema de dependencia cÃ­clica ("el huevo o la gallina") al usar una VIP (IP virtual) como punto de entrada al clÃºster.

---

## ğŸ›ï¸ Arquitectura

* **VIP del API Server**: `10.17.5.10`
* **VIP del Ingress**: `10.17.5.30`
* **Masters**:

  * `10.17.4.21` (bootstrap)
  * `10.17.4.22`
  * `10.17.4.23`
* **Workers**:

  * `10.17.4.24`, `10.17.4.25`, `10.17.4.26`, `10.17.4.27`
* **Load Balancers**:

  * `10.17.3.12`, `10.17.3.13`, `10.17.5.20`

---

## ğŸ”„ Orden de inicio esperado

1. El nodo `master1` se inicializa con su IP real (`10.17.4.21`).
2. Se levanta el `k3s-server` y el `etcd` en `master1`.
3. Los otros masters se unen usando `https://10.17.4.21:6443` (no la VIP).
4. Una vez el clÃºster estÃ¡ operativo:

   * Se configura la VIP (`10.17.5.10`) con Keepalived.
   * Se habilita HAProxy en los nodos `haproxy_keepalived`.
5. HAProxy redirige el trÃ¡fico de `10.17.5.10:6443` hacia los masters disponibles.
6. El `kubeconfig` puede comenzar a usar la VIP como endpoint oficial.

---

## âœ… ConfiguraciÃ³n correcta para romper el ciclo

### 1. **`master1` usa su IP real para bootstrap**

* El script de Ansible no apunta a la VIP (`10.17.5.10`) para levantar el nodo inicial.
* Esto permite iniciar el API Server antes que HAProxy.

### 2. **HAProxy permite `bind` en IPs no locales**

```yaml
- name: Habilitar net.ipv4.ip_nonlocal_bind
  ansible.posix.sysctl:
    name: net.ipv4.ip_nonlocal_bind
    value: "1"
    sysctl_file: /etc/sysctl.d/99-haproxy-nonlocal-bind.conf
    reload: yes
    state: present
```

Esto evita errores de HAProxy como `Cannot bind to VIP`, ya que permite iniciar el proceso sin que la IP estÃ© asignada localmente a la interfaz.

### 3. **Keepalived no requiere HAProxy para iniciar**

```ini
# override.conf
[Unit]
After=haproxy.service
# NO incluye Requires=haproxy.service
```

Esto asegura que Keepalived pueda arrancar independientemente de HAProxy. La relaciÃ³n es suave, no bloqueante.

### 4. **VIP solo se usa despuÃ©s de la estabilizaciÃ³n**

* El uso de la VIP para el `kubeconfig` solo se hace despuÃ©s de validar que el balanceador HAProxy estÃ© activo.

### 5. **ConfiguraciÃ³n de HAProxy**

El `haproxy.cfg` estÃ¡ correctamente estructurado para enrutar trÃ¡fico TCP en el puerto 6443 hacia los masters:

```haproxy
frontend kubernetes_api
    bind 10.17.5.10:6443
    mode tcp
    option tcplog
    default_backend kubernetes_masters

backend kubernetes_masters
    mode tcp
    balance roundrobin
    option tcp-check
    tcp-check connect port 6443
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server master-1 10.17.4.21:6443 check
    server master-2 10.17.4.22:6443 check
    server master-3 10.17.4.23:6443 check
```

---

## ğŸ”§ Validaciones adicionales

* Comando para verificar si HAProxy permite bind:

```bash
sysctl net.ipv4.ip_nonlocal_bind
```

* Verificar configuraciÃ³n de HAProxy:

```bash
haproxy -c -f /etc/haproxy/haproxy.cfg
```

* Verificar si estÃ¡ corriendo:

```bash
sudo systemctl status haproxy
```

---

## ğŸ§ª Estado de los Balanceadores tras el Playbook `install_haproxy_keepalived.yml`

Este es el estado esperado de los nodos balanceadores una vez finaliza la instalaciÃ³n automÃ¡tica con Ansible. Todos los nodos tienen HAProxy + Keepalived configurados, y las VIPs se asignan automÃ¡ticamente por prioridad.

| Hostname        | IP           | Rol                                 | Keepalived         | HAProxy             | VIPs Activas                      |
|-----------------|--------------|--------------------------------------|--------------------|---------------------|------------------------------------|
| `k8s-api-lb`    | `10.17.5.20` | Nodo principal de VIPs (`priority=100`) | âœ… Activo (MASTER)  | âœ… Activo y corriendo | âœ… `10.17.5.10` y `10.17.5.30`      |
| `loadbalancer1` | `10.17.3.12` | Respaldo 1 (`priority=120`)         | âœ… Activo (BACKUP)  | âœ… Activo (en espera) | âŒ (asumirÃ¡ VIPs si el principal cae) |
| `loadbalancer2` | `10.17.3.13` | Respaldo 2 (`priority=110`)         | âœ… Activo (BACKUP)  | âœ… Activo (en espera) | âŒ (asumirÃ¡ VIPs si los anteriores caen) |

---

### âš™ï¸ Detalles tÃ©cnicos

- Todos los nodos tienen `HAProxy` habilitado y en ejecuciÃ³n (`enabled + running`).
- Todos usan `ip_nonlocal_bind=1` para permitir el arranque sin poseer la VIP localmente.
- Keepalived gestiona la flotaciÃ³n de las siguientes IPs virtuales:
  - `10.17.5.10` â†’ Kubernetes API (`6443`)
  - `10.17.5.30` â†’ Ingress HTTP/HTTPS (`80` y `443`)
- El nodo que obtiene las VIPs es determinado por el archivo `host_vars/<ip>.yml` con sus prioridades:
  - `keepalived_priority_api`
  - `keepalived_priority_ingress`
- Si el nodo principal falla, el siguiente en prioridad **asume automÃ¡ticamente las VIPs** y el trÃ¡fico continÃºa sin interrupciones.

---


## ğŸ¯ ConclusiÃ³n

Con esta configuraciÃ³n:

* El nodo `master1` puede iniciar el clÃºster sin la VIP.
* Los nodos balanceadores y Keepalived pueden arrancar sin romper dependencias.
* HAProxy puede arrancar incluso si no posee la VIP.

ğŸ‘ EstÃ¡s aplicando correctamente un patrÃ³n de alta disponibilidad tolerante a fallos y circularidades de dependencia.



ansible-galaxy collection install community.general

ğŸ§° Resumen del Proyecto: HAProxy + Keepalived para K3s HA
Este proyecto implementa una soluciÃ³n de balanceo de carga altamente disponible para el acceso al clÃºster Kubernetes mediante HAProxy y Keepalived, gestionando mÃºltiples VIPs (IP Virtuales) para separar trÃ¡fico crÃ­tico del API y del Ingress HTTP/HTTPS.

ğŸ¯ Objetivo
Garantizar:

Acceso ininterrumpido al API de Kubernetes (puerto 6443).

Disponibilidad continua para trÃ¡fico HTTP/HTTPS hacia los servicios internos (Ingress).

Failover automÃ¡tico de las IPs virtuales entre mÃºltiples nodos balanceadores.

ğŸ”§ Componentes Clave
Componente	DescripciÃ³n
HAProxy	Balanceador de carga TCP/HTTP para API y trÃ¡fico web
Keepalived	Gestor de alta disponibilidad mediante VRRP para mover VIPs entre nodos
VIPs	IPs flotantes que garantizan un Ãºnico punto de entrada para el trÃ¡fico
Ansible	AutomatizaciÃ³n completa del despliegue y configuraciÃ³n
K3s	ClÃºster Kubernetes ligero y altamente disponible

ğŸŒ Arquitectura General de Red
bash
Copiar
Editar
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        Clientes externos    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                           â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                           â”‚ Cloudflareâ”‚
                           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                 â”‚
                       VPN / NAT / WireGuard
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Load Balancer Principalâ”‚  <- VIPs: 10.17.5.10 / 10.17.5.30
                    â”‚   (HAProxy + Keepalived)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  K3s Master #1  â”‚               â”‚  K3s Master #2-3    â”‚
     â”‚ API + etcd      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º API + etcd           â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

              â”‚ Ingress trÃ¡fico HTTP/HTTPS via Traefik
              â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Workers   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ“¦ VIPs y TrÃ¡fico
Tipo de TrÃ¡fico	Puerto(s)	VIP	Destino final
API de Kubernetes	6443	10.17.5.10	Nodos master de K3s (HA)
Servicios Ingress	80 / 443	10.17.5.30	Nodos worker vÃ­a Traefik (interno)

Las VIPs son asignadas dinÃ¡micamente al nodo con mayor prioridad activa.

Si un nodo falla, Keepalived transfiere la VIP al siguiente disponible.

ğŸ› ï¸ Mecanismo de Alta Disponibilidad
HAProxy:

ActÃºa como proxy TCP para 6443 y como proxy HTTP para 80/443.

Verifica salud de los nodos K3s y trabajadores.

Permite configuraciÃ³n nonlocal_bind para bindear IPs no locales.

Keepalived:

Ejecuta VRRP y scripts de tracking (estado de HAProxy).

Usa prioridad para determinar nodo MASTER de las VIPs.

Ejecuta vip_master.sh, vip_backup.sh, vip_fault.sh segÃºn evento.

Ansible:

Automatiza instalaciÃ³n en nodos HA.

Configura todos los archivos .cfg, .service, .conf necesarios.

Detecta Flatcar y aplica configuraciones especiales si es necesario.

ğŸ“‘ Flujo de ImplementaciÃ³n con Ansible
Detecta distribuciÃ³n (Flatcar o no).

Instala HAProxy, Keepalived y dependencias.

Configura sysctl para permitir nonlocal_bind.

Aplica configuraciones plantilladas (haproxy.cfg.j2, keepalived.conf.j2).

Configura override de systemd para evitar dependencias circulares.

Reinicia servicios y valida salud.

âœ… Ventajas Clave
Alta disponibilidad real (failover automÃ¡tico).

SeparaciÃ³n de trÃ¡fico crÃ­tico.

Escalabilidad horizontal simple.

ConfiguraciÃ³n 100% automatizada y auditable (IaC).

Seguridad de acceso por VPN y Cloudflare (si aplica).
---
ansible-k8s-ha-loadbalancer/
â”œâ”€â”€ ansible.cfg
â”œâ”€â”€ inventory/
â”‚   â””â”€â”€ hosts.ini
â”œâ”€â”€ host_vars/
â”‚   â”œâ”€â”€ 10.17.3.12.yml        # loadbalancer1
â”‚   â”œâ”€â”€ 10.17.3.13.yml        # loadbalancer2
â”‚   â””â”€â”€ 10.17.5.20.yml        # k8s-api-lb (nodo principal)
â”œâ”€â”€ playbooks/
â”‚   â””â”€â”€ install_haproxy_keepalived.yml
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ haproxy/
â”‚   â”‚   â””â”€â”€ haproxy.cfg.j2
â”‚   â””â”€â”€ keepalived/
â”‚       â””â”€â”€ keepalived.conf.j2
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
---


